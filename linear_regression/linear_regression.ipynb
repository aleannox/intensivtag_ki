{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensivtag KI: Hands-On Lineare Regression\n",
    "### Overfitting am Beispiel von PayPals Zahlungsvolumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Praxisteil werden wir am Beispiel von PayPals Zahlungsvolumen das Phänomen des Overfittings untersuchen, das für Data Science und Machine Learning von zentraler Bedeutung ist. Wir werden auch die Notwendigkeit der Unterscheidung zwischen Trainings- und Testdaten kennenlernen.\n",
    "\n",
    "Ziel des Praxisteils ist es, ein Modell zu entwickeln, das zukünftige Zahlungsvolumina möglichst gut prognostiziert.\n",
    "\n",
    "Um die Genauigkeit der Modellprognose zu beurteilen, müssen wir das Modell auf Daten auswerten, die nicht in die Modellentwicklung eingeflossen sind. Warum ist das so? \n",
    "\n",
    "Der Sinn und Zweck eines Modells ist in der Regel die Möglichkeit, für Daten, die zum Zeitpunkt der Modellentwicklung noch nicht bekannt sind, Prognosen zu berechnen. Im vorliegenden Beispiel möchten wir auf Grundlage von Zahlungsvolumina in vergangenen Quartalen die entsprechenden Volumina für zukünftige Quartale prognostizieren. Ein Modell, das die Vergangenheit perfekt beschreibt, aber bei der Prognose der Zukunft fehlschlägt, ist für uns also wertlos. \n",
    "\n",
    "Wir brauchen folglich eine Möglichkeit, die Genauigkeit der Modellprognose <i>abzuschätzen</i> - genau berechnen können wir sie ja naturgemäß nicht. Dazu gehen wir wie folgt vor. Wir teilen die vorhandenen Daten auf in einen <i>Trainings-</i> und einen <i>Testdatensatz</i>. Die Aufteilung nehmen wir so vor, dass die Testdaten zeitlich gesehen in der Zukunft der Trainingsdaten liegen. Wir trainieren das Modell auf den Trainingsdaten und werten es auf den Testdaten aus. Wir simulieren also sozusagen eine Zukunftsprognose.\n",
    "\n",
    "Overfitting ist einer der möglichen Gründe für die bereits angesprochene ungünstige Situation, dass ein Modell die Trainingsdaten sehr gut beschreibt, aber auf den Testdaten versagt. Wir werden im folgenen sehen, wie Overfitting im Extremfall aussehen kann. Im nachfolgenden Theorieteil werden wir besprechen, wie sich Overfitting ganz allgemein vermeiden lässt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Importe\n",
    "\n",
    "Als erstes importieren wir die benötigten Funktionen aus Hilfsdateien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression_utils import load_data, plot_data, train_test_split, train_model, evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Die Daten\n",
    "\n",
    "Wir laden nun die Daten aus einer CSV-Datei und visualisieren sie in ihrer Reinform ohne ein Interpretationsmodell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data()\n",
    "plot_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Prognosequalität unserer Modelle zu abzuschätzen, teilen wir den Datensatz wie besprochen in einen Trainings- und einen Testdatensatz auf. Wir nutzen als Testdaten zunächst die letzten 25% der Daten. Später kannst du mit anderen Optionen experimentieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = train_test_split(x, y, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Ein lineares Modell\n",
    "\n",
    "Die Daten zeigen einen klaren Aufwärtstrend. Dieser lässt sich am einfachsten durch ein lineares Modell - eine Gerade - modellieren.\n",
    "\n",
    "Wir sehen, dass das lineare Modell die Trainingsdaten im Wesentlichen gut beschreibt, die Prognose auf den Testdaten aber sichtbar abweicht. Diese Beobachtung unterstreicht gleichzeitig die Notwendigkeit des Training-/Test-Splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Modells\n",
    "linear_model = train_model(x_train, y_train, 1)\n",
    "\n",
    "# Auswertung des Modells\n",
    "evaluate_model(linear_model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Polynomiale Modelle\n",
    "\n",
    "Eine Möglichkeit, die Trainingsdaten genauer zu modellieren und bessere Prognosen auf den Testdaten zu erhalten, ist Polynome höheren Grades zu betrachten, z.B. eine Parabel.\n",
    "\n",
    "Wir sehen, dass eine Parabel die Trainingsdaten besser als eine Gerade modelliert, und eine deutlich genauere Prognose auf den Testdaten liefert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Modells\n",
    "order_2_model = train_model(x_train, y_train, 2)\n",
    "\n",
    "# Auswertung des Modells, Vergleich mit dem linearen Modell von zuvor\n",
    "evaluate_model(order_2_model, x_train, y_train, x_test, y_test, linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je höher der Grad des im Modell verwendeten Polynoms ist, umso besser kann das Modell die vorhandenen Schwankungen in den Trainingsdaten abbilden. Darum hören wir nicht bei der Parabel auf, sondern probieren höhere Polynomgrade aus.\n",
    "\n",
    "Im folgenden Beispiel nutzen wir ein Polynom 6. Grades. Es schmiegt sich besser an die Trainingsdaten an als eine Parabel, offensichtlich auf Kosten der Prognosequalität auf den Testdaten. Grund dafür ist die Tatsache, dass die vom Modell beschriebenen Schwankungen kein globaler Trend, sondern nur kurzfristige Fluktuationen sind. Das Modellieren dieser Fluktuationen ist ein Beispiel für Overfitting: die Trainingsdaten werden durch immer komplexere Modelle immer besser beschrieben, die Testdaten durch dieselben Modelle allerdings immer schlechter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Modells\n",
    "order_6_model = train_model(x_train, y_train, 6)\n",
    "\n",
    "# Auswertung des Modells, Vergleich mit der Parabel von zuvor\n",
    "evaluate_model(order_6_model, x_train, y_train, x_test, y_test, order_2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am deutlichsten sichtbar wird das Overfitting, wenn wir ein Polynom wählen, dessen Grad derart ist, dass es die Trainingsdaten perfekt modelliert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestimmen des Polynomgrads, dass die Trainingsdaten perfekt modelliert\n",
    "# (Die Formel lässt sich mathematisch begründen.)\n",
    "perfect_polynomial_degree = len(x_train) - 1\n",
    "\n",
    "# Trainieren des Modells\n",
    "perfect_model = train_model(x_train, y_train, perfect_polynomial_degree)\n",
    "\n",
    "# Auswertung des Modells, Vergleich mit der Parabel von zuvor\n",
    "evaluate_model(perfect_model, x_train, y_train, x_test, y_test, order_2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es scheint so zu sein, dass ein Polynom 2. Grades einen guten Kompromiss zwischen Modellqualität auf den Trainingsdaten und Modellqualität auf den Testdaten ergibt. Im nachfolgenden Theorieteil werden wir besprechen, wie sich solch ein Kompromiss - der Overfitting vermeidet - systematisch finden lässt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Experimente\n",
    "\n",
    "Im Folgenden könnt ihr nach Lust und Laune mit den Modellparametern experimentieren! Ihr könnt den Grad des Polynoms wählen sowie die Aufteilung der Daten in Trainings- und Testdaten bestimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad des Polynoms\n",
    "my_polynomial_degree = 9\n",
    "\n",
    "# Grad des Polynoms - Vergleichsmodell\n",
    "my_polynomial_degree_compare = 3\n",
    "\n",
    "# Relative Größe des Testdatensatzes\n",
    "my_test_size = 0.5\n",
    "\n",
    "# Trainings-/Test Split\n",
    "my_x_train, my_y_train, my_x_test, my_y_test = train_test_split(x, y, my_test_size)\n",
    "\n",
    "# Trainieren des Modells und des Vergleichsmodells\n",
    "my_model = train_model(my_x_train, my_y_train, my_polynomial_degree)\n",
    "my_model_compare = train_model(my_x_train, my_y_train, my_polynomial_degree_compare)\n",
    "\n",
    "# Auswertung des Modells und des Vergleichsmodells\n",
    "evaluate_model(my_model, my_x_train, my_y_train, my_x_test, my_y_test, my_model_compare)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
