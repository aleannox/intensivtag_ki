{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in das Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundsätzlicher Ansatz des Clusterings\n",
    "\n",
    "<img style=\"float: right; padding-left: 10%; padding-right: 5%;\" src=\"images/example.png\" width=\"40%\">\n",
    "\n",
    "### Aufgabenstellung\n",
    "* Gegeben eine Menge an Datenpunkten\n",
    "* Erkenne interessante Strukturen in den Daten\n",
    "* Cluster = Menge von Datenpunkten die untereinander ähnlich sind, sich aber von den anderen Datenpunkten unterscheiden\n",
    "\n",
    "### Zentrale Fragestellung\n",
    "**Was bedeutet es, dass Datenpunkte \"ähnlich\" sind?**  \n",
    "&rarr; Keine universelle Antwort  \n",
    "&rarr; Kein universell gültiges Clustering\n",
    "\n",
    "### Beispiele\n",
    "* Kundenprofile\n",
    "* Fehlerkonstellationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; padding-left: 50px; padding-right: 50px;\" src=\"images/kmeans.png\" width=\"50%\">\n",
    "\n",
    "Wir schauen uns nun **K-Means** an, das am häufigstens eingesetzte Clusteringverfahren.\n",
    "\n",
    "* K-Means ist universell verwendbar\n",
    "* Es basiert auf der Berechnung sogenannter Zentroide\n",
    "* Die Zentroide entsprechen den Clustern\n",
    "* Datenpunkte werden den jeweils nächsten Zentroiden zugeordnet\n",
    "* Parameter: Anzahl der Cluster\n",
    "* Das Verfahren läuft sehr schnell\n",
    "* Die resultierenden Cluster sind konvex\n",
    "* Eigentlich ein Algorithmus zur Partitionierung\n",
    "* Clustering ist vollständig: jeder Datenpunkt wird einem Cluster zugeordnet  \n",
    "&rarr; Das gilt allerdings auch für fehlerhafte Datenpunkte und Ausreißer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun schauen wir uns K-Means in Aktion an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "\n",
    "import seaborn as snss\n",
    "\n",
    "style = {'description_width': '150px'}\n",
    "layout = widgets.Layout(width='400px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vier synthetische Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung und Visualisierung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Funktionsweise des Clusterings zu prüfen, beginnen wir zuerst mit künstlich generierten Daten. Diese werden in vier Clustern verteilt, die um die Punkte (-1, -1), (-1, 1), (1, -1), (1, 1) zentriert sind und sich überlappen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "data = np.concatenate(\n",
    "    [np.random.multivariate_normal([x, y], np.diag([.3, .3]), size=n)\n",
    "     for x in [-1, 1]\n",
    "     for y in [-1, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['x', 'y'])\n",
    "_ = df.plot(kind='scatter', x='x', y='y', s=40, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsprinzip des Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Means arbeitet iterativ:\n",
    "1. Es werden _k_ Cluster-Repräsentanten (Zentroiden oder Centroids) initial ausgewählt. Diese Auswahl erfolgt je nach Implementierung rein zufällig oder (häufiger) anhand einer Auswahlheuristik zwecks schnellerer Konvergenz des Verfahrens.\n",
    "2. Alle übrigen Datenpunkte werden dem Repräsentanten zugeordnet, dem sie - entsprechend dem definierten Ähnlichkeitsmaß - am nächsten sind.\n",
    "3. Ein neues Set von _k_ Cluster-Repräsentanten wird erzeugt, indem für jedes Cluster der Durchschnittsvektor (Mean) der Attributwerte aller dem Cluster im vorhergehenden Schritt zugeordneten Datenpunkte berechnet wird.\n",
    "4. Schritte 2 und 3 werden solange iterativ wiederholt, bis entweder eine vordefinierte Anzal $n_{max}$ von Iterationen durchlaufen wurde, oder in der $n$-ten Durchführung von Schritt 2 kein Datenpunkt mehr einem anderen Cluster zugeordnet wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun definieren wir - ähnlich wie oben - einige Hilfsfunktionen, mit der wir interaktiv bestimmen und plotten können, wie das Zwischenergebnis nach einer vorgegebenen Anzahl von Iterationsschritten aussieht. Einige Schlüsselstellen im Code sind per Kommentar hervorgehoben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot mit Einfärbung der Punkte entsprechend ihrer Clusterzugehörigkeit,\n",
    "# Zentroiden als schwarze Kreuze überlagert \n",
    "def plot_clustering_steps(df, clusterer, print_number=False):\n",
    "    df = df.copy()\n",
    "    df['cid'] = clusterer.fit_predict(df[['x', 'y']]) # Cluster(zwischen)ergebnis anhand übergeber Einstellung berechnen\n",
    "    n_clusters = df['cid'].max() + 1\n",
    "    n_outliers = np.sum(df['cid'] == -1)\n",
    "    if print_number:\n",
    "        print(f'number of clusters: {n_clusters}\\nnumber of outliers: {n_outliers}')\n",
    "    cmap = plt.get_cmap('Set1', n_clusters+1) # Einheitliche, vordefinierte Farbskala\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plotten der Entscheidungsgrenzen der Zuordnung einzelner Punkte zu Zentroiden\n",
    "    voro = Voronoi(clusterer.cluster_centers_)\n",
    "    voronoi_plot_2d(voro, ax, show_points=False, show_vertices=False)\n",
    "    \n",
    "    # Plotten der Datenpunkte mit Einfärbung\n",
    "    df.plot(ax=ax, kind='scatter', x='x', y='y', c='cid', cmap=cmap, s=40, colorbar=False, figsize=(6, 6))\n",
    "    \n",
    "    # Überlagern der Zentroiden als schwarze X-Symbole\n",
    "    for c in clusterer.cluster_centers_:\n",
    "        ax.plot(c[0], c[1], 'kX')\n",
    "        \n",
    "    ax.set_xlim(-3.0, 3.0)\n",
    "    ax.set_ylim(-3.0, 3.0)\n",
    "    \n",
    "# k-Means mit fest vorgegebenen Inital-Zentroiden und bis zu einer vorgegebenen Anzahl Durchläufe berechnen\n",
    "def stepped_kmeans(k, steps):\n",
    "    kmeans = KMeans(n_clusters=k,\n",
    "                    init=df[:k].to_numpy(), # Wähle die ersten k Zeilen des Dataframe als initiale Zentroiden\n",
    "                    n_init=1, # Da die Zentroiden feststehen wird keine Auswahlheuristik benötigt\n",
    "                    max_iter=steps   # Erzwinge Abbruch des Verfahrens nach der festglegten Anzahl Durchläufe\n",
    "                   )\n",
    "    plot_clustering_steps(df, clusterer=kmeans) # (Zwischen-)Ergebnis plotten\n",
    "\n",
    "    \n",
    "# Interaktionselemente mit der Berechnungs- und Plotfunktion verbinden\n",
    "_ = interact(\n",
    "    stepped_kmeans,\n",
    "    k=widgets.SelectionSlider(\n",
    "        value=4,\n",
    "        options=range(3,15),\n",
    "        layout=layout,\n",
    "        style=style,\n",
    "        description='Number of clusters (k)',\n",
    "        orientation='horizontal'\n",
    "    ),\n",
    "    steps=widgets.SelectionSlider(\n",
    "        options=range(1,15),\n",
    "        layout=layout,\n",
    "        style=style,\n",
    "        description='Number of iterations',\n",
    "        orientation='horizontal'\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir berechnen nun die zugehörige Silhouetten-Kurve. Wie man sehen kann, wird der höchste Wert bei 4, also der korrekten Anzahl der Cluster, erreicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sil_values = []\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    clust = kmeans.fit_predict(df[['x', 'y']])\n",
    "    sil_values.append(silhouette_score(df[['x', 'y']], clust))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2,20),sil_values)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_xticks(range(2, 20, 2))\n",
    "ax.set_ylabel('Silhouette value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaillierter Silhouette Plot\n",
    "Code snippet aus der scikit-learn Bibliothek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=1,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example we\n",
    "    # limit display to within [-0.2, 1]\n",
    "    ax1.set_xlim([-0.2, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grenzen von bzw. Annahmen hinter k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Beispiele aus den mit skLearn ausgelieferten Testdatensätze bzw. Dokumentation sollen Situationen veranschaulichen, in denen k-Means unintuitive und möglicherweise unerwartete Cluster erzeugt. In den ersten drei Diagrammen entsprechen die Eingabedaten nicht den impliziten Annahmen, welche k-Means zugrunde liegen, und als Ergebnis unerwünschte Cluster erzeugt werden. Im letzten Plot gibt k-means trotz ungleichmäßig großer Blobs intuitive Cluster zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Falsch gewähltes K = \"falsche\" Anzahl Blobs \n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"Falsches k ungleich der Anzahl Blobs\")\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
    "plt.title(\"Anisotropish verteilte, nicht kugelförmige Blobs\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"Stark unterschiedliche Varianz (Punktdichte)\")\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3,\n",
    "                random_state=random_state).fit_predict(X_filtered)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n",
    "plt.title(\"Blobs ungleicher Größe\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris-Datenset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes überprüfen wir die Funktionsweise von k-Means an einem echten Datensatz, dem berühmten Iris-Datensatz von Ronald Fisher (https://en.wikipedia.org/wiki/Iris_flower_data_set). Hierbei handelt es sich um Messungen von Blättern für unterschiedliche Arten aus der Gattung Iris (Schwertlilien).\n",
    "\n",
    "Dieses wird zuerst aus der Datei \"iris.csv\" eingelesen und dann visualisiert. Dabei sind auf der X-Achse die Breite der Kelchblätter und auf der Y-Achse die Länge der Blütenblätter aufgetragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.environ['DATASET_PATH']\n",
    "iris_df = pd.read_csv(dataset_path + '/Iris/iris.csv'). \\\n",
    "        rename({'sepal length (cm)': 's', 'sepal width (cm)': 'x', 'petal length (cm)': 'y', 'petal width (cm)': 'p'}, axis=1)\n",
    "_ = iris_df.plot(kind='scatter', x='x', y='y', s=40, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering mit k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_clustering(df, clusterer, print_number=False):\n",
    "    df = df.copy()\n",
    "    df['clusterid'] = clusterer.fit_predict(df)\n",
    "    n_clusters = df['clusterid'].max() + 1\n",
    "    n_outliers = np.sum(df['clusterid'] == -1)\n",
    "    if print_number:\n",
    "        print(f'number of clusters: {n_clusters}\\nnumber of outliers: {n_outliers}')\n",
    "    cmap = plt.get_cmap('Set1', n_clusters+1)\n",
    "    ax = df.plot(kind='scatter', x='x', y='y', c='clusterid', cmap=cmap, s=40, colorbar=False, figsize=(6, 6))\n",
    "    ax.grid()\n",
    "\n",
    "def plot_kmeans(k = 3):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    fit_and_plot_clustering(iris_df[['x', 'y']], clusterer=kmeans)\n",
    "    \n",
    "_ = interact(\n",
    "    plot_kmeans,\n",
    "    k=widgets.SelectionSlider(\n",
    "        options=range(1, 10),\n",
    "        description='Number of clusters (k)',\n",
    "        layout=layout,\n",
    "        style=style,\n",
    "        orientation='horizontal'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier erzeugen wir ein Silhouetten-Bild. Das Maximum liegt hier bei zwei Clustern. Dieser Wert scheint auf Basis der Daten auch visuell plausibel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_values = []\n",
    "for k in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    clust = kmeans.fit_predict(iris_df[['x', 'y']])\n",
    "    sil_values.append(silhouette_score(iris_df[['x', 'y']], clust))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2,10),sil_values)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_xticks(range(2,10, 2))\n",
    "ax.set_ylabel('Silhouette value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Auflösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich entstammen die Daten jedoch *drei* verschiedenen Arten. Dies sehen wir, wenn wir die Spalte mit der Art hinzunehmen und die Punkte farbig kennzeichnen. Man sieht, dass die rote Art gut getrennt ist, jedoch die organene und graue Art ineinander übergehen. Diese Schwierigkeit bei der Separierung ist in der Praxis häufig anzutreffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = iris_df.plot(kind='scatter',\n",
    "            x='x',\n",
    "            y='y',\n",
    "            s=40,\n",
    "            c='class',\n",
    "            cmap=plt.get_cmap('Set1'),\n",
    "            colorbar=False,\n",
    "            figsize=(6, 6)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorteile und Nachteile von K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Vorteile | Nachteile |\n",
    "| -------- | ----------|\n",
    "| Sehr schnell und gut skalierbar | Empfindlich gegenüber Ausreißern |\n",
    "| Neue Punkte können ohne Neutraining einem Cluster zugeordnet werden | Anzahl der Cluster muss vorgegeben werden |\n",
    "| Clusterzuordnung kann als zusätzliches Feature für andere Modelle genutzt werden | Wenig flexibel in Bezug auf die Clustergröße: Alle Cluster sind mehr oder minder kugelförmig und ähnlich groß |\n",
    "| Universell einsetzbar | Ergebnis hängt auch von der (zufälligen) Initialisierung ab &rarr; am besten mehrfach starten und die beste Variante wählen |\n",
    "| Einfach zu verstehen | Clusterrepräsentanten (Zentroide) sind künstlich und nicht Teil der Daten |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchisches Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; padding-left: 10%; padding-right: 5%;\" src=\"images/hierarchical.png\" width=\"55%\">\n",
    "\n",
    "### Agglomeratives Clustering\n",
    "* Bottom-up\n",
    "* Beginnt mit Clustern der Größe eins\n",
    "* Cluster werden iterativ zusammengeführt  \n",
    "&rarr; grüner Pfeil\n",
    "\n",
    "### Aufspaltendes Clustering\n",
    "* Top-down\n",
    "* Beginnt mit allen Daten in einem Cluster\n",
    "* Spaltet iterativ die Cluster auf  \n",
    "&rarr; orangener Pfeil\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/hierarchical_points.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionsweise\n",
    "* Wähle eine Distanzfunktion für Cluster (Linkage-Funktion)\n",
    "* Jeder Datenpunkt startet als Einpunkt-Cluster\n",
    "* Bestimme die beiden Cluster mit dem geringsten Wert der Linkage-Funktion und füge sie zusammen\n",
    "* Wiederhole, bis nur noch ein Cluster übrig ist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; padding-left: 10%; padding-right: 5%;\" src=\"images/threshold.png\" width=\"55%\">\n",
    "\n",
    "### Ergebnis\n",
    "* Gibt eine Hierarchie von Clustern zurück\n",
    "* Wird in einem Dendrogramm dargestellt\n",
    "* Knoten sind Datenpunkte, jede Zeile entspricht einer Clusterzusammenfassung\n",
    "* Gewünschte Clusteraufteilung erhält man, indem man eine Schwelle im Baum festlegt  \n",
    "&rarr; z.B. mit dem \"elbow heterogeneity criterion\"\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/threshold_clusters.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linkage-Funktionen\n",
    "\n",
    "Die Linkage-Funktion zur Zusammenführung der Cluster kann frei gewählt werden. Die beliebesten Linkage-Funktionen sind Single, Average, Complete und Ward Linkage.\n",
    "\n",
    "<img src=\"images/linkage.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erproben nun das agglomerative Clustering mit dem Iris-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_hierarchical(k, linkage):\n",
    "    agg = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "    fit_and_plot_clustering(iris_df[['x', 'y']], clusterer=agg)\n",
    "    \n",
    "_ = interact(\n",
    "    plot_hierarchical,\n",
    "    k=widgets.SelectionSlider(\n",
    "        options=range(1, len(iris_df)),\n",
    "        description='Number of clusters (k)',\n",
    "        layout=layout,\n",
    "        style=style,\n",
    "        orientation='horizontal'\n",
    "    ),\n",
    "    linkage=widgets.Dropdown(\n",
    "        options=['ward', 'complete', 'average', 'single'],\n",
    "        layout=layout,\n",
    "        style=style\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorteile und Nachteile des agglomerativen Clusterings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Vorteile | Nachteile |\n",
    "| -------- | ----------|\n",
    "| Kommt gut mit vielen Clustern zurecht | Berechnet die vollständige Hierarchie, auch wenn nur wenige Cluster gewünscht sind |\n",
    "| Funktioniert auch bei unterschiedlich großen Clustern gut | Funktioniert nicht gut mit komplex geformten Clustern |\n",
    "| Wahlmöglichkeit in Bezug auf Auswahl der Cluster und die Linkage-Funktion | Berechnung der Linkage-Funktion rechenaufwändig |\n",
    "| Manche Implementierungen erlauben es, Vorwissen dazu einzubringen, welche Datenpunkte zu unterschiedlichen Clustern gehören | Bei Single-Linkage Neigung, Ketten zu bilden |\n",
    "| Einfache Beurteilung des Clusterings in Abhängigkeit von der Heterogenitäts-Schwelle | Wahl einer bestimmten Heterogenitäts-Schwelle nötig |\n",
    "| Ausreißer werden implizit identifiziert | Neue Punkte können nicht einem Cluster zugeordnet werden, ohne das Modell neu zu berechnen |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bessere Lösung für den Iris-Datensatz mit K-Means im Vierdimensionalen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall liegt es auch daran, dass wir nur zwei der beschreibenden Attribute im Clustering berücksichtigt haben - die 'sepal width (cm)' als 'x' sowie die 'petal length (cm)' als 'y', siehe die erste Code-Zelle unter Abschnitt 1.3. Diese beiden Dimensionen sind jene, die wir auch zum Zeichnen des zweidimensionalen Scatterplots herangezogen haben. Der k-Means Algorithmus ist jedoch sehr wohl in der Lage, mit deutlich mehr als zwei Dimensionen umzugehen. Der Datensatz enthält tatsächlich vier Dimensionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bauen wir also ein weiteres Cluster-Modell, welches alle vier Dimensionen berücksichtigt. Dabei setzen wir k=3, entsprechend der offiziellen Einteilung der Iris in drei Unterarten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_all = KMeans(n_clusters=3) # Cluster\n",
    "iris_df_cid = kmeans_all.fit_predict(iris_df[['s', 'x', 'y', 'p']]) # Nutze alle Attribute. Zuvor: Selektion von nur zwei Attributen mittels df[['x', 'y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das resultierende Clustering trifft die Einteilung der Biologen sehr viel besser, wenn auch nicht perfekt (links die Einteilung der Biologen, rechts das Ergebnis des Clustermodells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "iris_df.plot(ax=axes[0],\n",
    "        kind='scatter',\n",
    "        x='x',\n",
    "        y='y',\n",
    "        s=40,\n",
    "        c='class', # Farbe entsprechend der vorgegebenen 'class'\n",
    "        cmap=plt.get_cmap('Set1'),\n",
    "        colorbar=False,\n",
    "        figsize=(14, 6)\n",
    "        )\n",
    "_ = iris_df.plot(ax=axes[1],\n",
    "        kind='scatter',\n",
    "        x='x',\n",
    "        y='y',\n",
    "        s=40,\n",
    "        c=iris_df_cid, # Farbe entsprechend der berechneten Cluster-ID\n",
    "        cmap=plt.get_cmap('Set1'),\n",
    "        colorbar=False,\n",
    "        figsize=(14, 6)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der bisher gewählten Projektion in die gewählten zwei Dimensionen x, y ist nicht ersichtlich, warum sich die grauen von den gelben Instanzen trennen lassen. Mit einer sogenannten Scatterplot-Matrix, welche systematisch alle möglichen Kombinationen von zwei (aus insgesamt vier) Dimensionen gegeneinander darstellt, lässt sich diese Trennung im hochdimensionalen Datenraum besser erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(iris_df[['s', 'x', 'y', 'p']],\n",
    "               marker='o',\n",
    "               alpha=1.0,\n",
    "               c=iris_df_cid,\n",
    "               cmap=plt.get_cmap('Set1'),\n",
    "               figsize=(16, 16),\n",
    "               diagonal=\"hist\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aber auch mit vier Dimensionen liefert der Scatter-Plot nicht die von Biologen als richtig angesehene Trennung in drei Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_values = []\n",
    "for k in range(2,10):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    clust = kmeans.fit_predict(iris_df[['s', 'x', 'y', 'p']])\n",
    "    sil_values.append(silhouette_score(iris_df[['s', 'x', 'y', 'p']], clust))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2,10),sil_values)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_xticks(range(2,10, 2))\n",
    "ax.set_ylabel('Silhouette value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
